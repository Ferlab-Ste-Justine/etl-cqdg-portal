args=[]
sources=[
    {
        format=AVRO
        id="raw_patient"
        keys=[]
        loadtype=OverWrite
        partitionby=[
            "study_id",
            "release_id"
        ]
        path="/fhir/patient"
        readoptions {}
        storageid=storage
        table {
            database=normalized
            name="raw_patient"
        }
        writeoptions {
            "created_on_column"="created_on"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    },
        {
            format=CSV
            id="raw_patient"
            keys=[]
            loadtype=OverWrite
            partitionby=[
                "study_id",
                "release_id"
            ]
            path="/fhir/patientcsv"
            readoptions {}
            storageid=storage
            table {
                database=normalized
                name="raw_patient"
            }
            writeoptions {
                "created_on_column"="created_on"
                "updated_on_column"="updated_on"
                "valid_from_column"="valid_from"
                "valid_to_column"="valid_to"
            }
        },
    {
        format=DELTA
        id="normalized_patient"
        keys=[]
        loadtype=OverWritePartition
        partitionby=[
            "study_id",
            "release_id"
        ]
        path="/normalized/patient"
        readoptions {}
        storageid=storage
        table {
            database=normalized
            name=patient
        }
        writeoptions {
            "created_on_column"="created_on"
            overwriteSchema="true"
            "updated_on_column"="updated_on"
            "valid_from_column"="valid_from"
            "valid_to_column"="valid_to"
        }
    }
]
sparkconf {
    "spark.databricks.delta.retentionDurationCheck.enabled"="false"
    "spark.delta.merge.repartitionBeforeWrite"="true"
    "spark.fhir.server.url"="http://localhost:8080"
    "spark.hadoop.fs.s3a.access.key"=${?AWS_ACCESS_KEY}
    "spark.hadoop.fs.s3a.endpoint"=${?AWS_ENDPOINT}
    "spark.hadoop.fs.s3a.impl"="org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access"="true"
    "spark.hadoop.fs.s3a.secret.key"=${?AWS_SECRET_KEY}
    "spark.master"=local
    "spark.sql.catalog.spark_catalog"="org.apache.spark.sql.delta.catalog.DeltaCatalog"
    "spark.sql.extensions"="io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.legacy.timeParserPolicy"=CORRECTED
    "spark.sql.mapKeyDedupPolicy"="LAST_WIN"
}
storages=[
    {
        filesystem=S3
        id=storage
        path="s3a://cqdg-qa-app-clinical-data-service"
    }
]
